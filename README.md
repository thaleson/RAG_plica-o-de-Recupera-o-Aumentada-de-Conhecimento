# ğŸ“š AplicaÃ§Ã£o de RecuperaÃ§Ã£o Aumentada de Conhecimento (RAG) com Ollama 3

Bem-vindo ao projeto de RecuperaÃ§Ã£o Aumentada de Conhecimento (RAG) usando o modelo Ollama 3! ğŸ‰ Este projeto oferece uma soluÃ§Ã£o robusta para extrair e consultar informaÃ§Ãµes de arquivos PDF. Vamos comeÃ§ar! ğŸš€

## ğŸŒŸ Funcionalidades

- **ExtraÃ§Ã£o de Texto:** Extraia texto de arquivos PDF de maneira eficiente. ğŸ“„
- **Armazenamento e RecuperaÃ§Ã£o:** Armazene e recupere informaÃ§Ãµes extraÃ­das de PDFs. ğŸ—ƒï¸
- **Consultas Inteligentes:** Realize consultas e obtenha respostas precisas baseadas no conteÃºdo do PDF. ğŸ¤–

## ğŸ“‹ Requisitos

Certifique-se de ter os seguintes pacotes instalados:

- **Python 3.10 ou superior** ğŸ
- **Ollama 3** ğŸ“¦
- **ChromaDB** ğŸ“Š
- **PyMuPDF** ğŸ“š

## ğŸ“¥ InstalaÃ§Ã£o

### 1. Clone o RepositÃ³rio

```bash
git clone https://github.com/thaleson/case_starsoft
cd seu_repositorio
```

### 2. Crie e Ative um Ambiente Virtual

- **No Windows:**

  ```bash
  python -m venv env
  .\env\Scripts\activate
  ```

- **No Linux/macOS:**

  ```bash
  python3 -m venv env
  source env/bin/activate
  ```

### 3. Instale as DependÃªncias

Execute:

```bash
pip install -r requirements.txt
```



## ğŸ› ï¸ ConfiguraÃ§Ã£o do Ambiente

Para executar a aplicaÃ§Ã£o, vocÃª precisarÃ¡ configurar o ambiente corretamente. Siga as etapas abaixo:

### 1. **Instalar o Servidor Ollama**

Antes de rodar a aplicaÃ§Ã£o, vocÃª deve instalar e configurar o servidor Ollama. Siga as instruÃ§Ãµes abaixo:

1. Baixe e instale o servidor Ollama [aqui](https://ollama.com/download) (Certifique-se de escolher a versÃ£o apropriada para o seu sistema operacional).
2. ApÃ³s a instalaÃ§Ã£o, inicie o servidor Ollama no seu sistema.

### 2. **Baixar o Modelo Llama 3**

A aplicaÃ§Ã£o requer o modelo Llama 3 para funcionar corretamente. Siga os passos abaixo para baixÃ¡-lo:


Aqui estÃ£o os passos para baixar o modelo Llama 3:

1. **Verifique se o `ollama` estÃ¡ instalado:**
   ```bash
   ollama --version
   ```
   Se vocÃª ainda nÃ£o tiver o `ollama`, pode instalar seguindo as instruÃ§Ãµes no [site oficial](https://ollama.com/).

2. **Baixe o modelo Llama 3:**
   Se vocÃª jÃ¡ tem o `ollama` instalado, vocÃª pode baixar o modelo usando o seguinte comando no terminal:
   ```bash
   ollama pull llama3
   ```
   Este comando faz o download do modelo Llama 3 e o instala no seu ambiente.

3. **Verifique a instalaÃ§Ã£o:**
   ApÃ³s o download, vocÃª pode verificar se o modelo estÃ¡ disponÃ­vel com:
   ```bash
   ollama models
   ```
   Isso deve listar todos os modelos disponÃ­veis, incluindo o Llama 3.

Se vocÃª tiver algum problema durante o processo, pode verificar a documentaÃ§Ã£o do `ollama` ou o suporte tÃ©cnico do serviÃ§o.
   
### 3. **Executar a AplicaÃ§Ã£o**

ApÃ³s configurar o servidor Ollama e baixar o modelo Llama 3, vocÃª pode iniciar a aplicaÃ§Ã£o. Certifique-se de que o servidor Ollama estÃ¡ rodando antes de executar a aplicaÃ§Ã£o.

Para executar a aplicaÃ§Ã£o, use o comando:

```bash
streamlit run app.py
```


## ğŸ³ Uso com Docker

Se preferir usar Docker para rodar a aplicaÃ§Ã£o, siga os passos abaixo:

### 1. Crie o Dockerfile

Certifique-se de que o `Dockerfile` no diretÃ³rio raiz do projeto estÃ¡ configurado corretamente. Exemplo de `Dockerfile`:

```dockerfile
# Use a imagem base oficial do Python
FROM python:3.10-slim

# Define o diretÃ³rio de trabalho dentro do container
WORKDIR /app

# Copia o arquivo de dependÃªncias
COPY requirements.txt .

# Instala as dependÃªncias do projeto
RUN pip install --no-cache-dir -r requirements.txt

# Copia todo o conteÃºdo do projeto para o diretÃ³rio de trabalho no container
COPY . .

# ExposiÃ§Ã£o da porta usada pelo Streamlit
EXPOSE 8501

# Comando para iniciar a aplicaÃ§Ã£o Streamlit
CMD ["streamlit", "run", "main.py"]
```

### 2. Crie o arquivo `docker-compose.yml`

Configure o arquivo `docker-compose.yml` para incluir o Ollama e o ChromaDB. Exemplo:

```yaml
version: '3.8'

services:
  app:
    build:
      context: .
      dockerfile: Dockerfile
    ports:
      - "8501:8501"
    volumes:
      - ./data:/app/data
      - ./db:/app/db
    environment:
      - STREAMLIT_SERVER_ENABLE_CORS=false
      - STREAMLIT_SERVER_HEADLESS=true
    depends_on:
      - chromadb
      - ollama

  chromadb:
    image: nouchka/sqlite3
    ports:
      - "8000:8000"
    volumes:
      - ./db:/data
    environment:
      - SQLITE_DATABASE=/data/chroma.sqlite3

  ollama:
    image: your-ollama-image
    ports:
      - "11434:11434"
    environment:
      - OLLAMA_MODEL=llama3
    volumes:
      - ./models:/models
```

### 3. Inicie os ContÃªineres

Execute:

```bash
docker-compose up
```

Certifique-se de que o contÃªiner do Ollama estÃ¡ configurado para rodar o modelo `llama3` e estÃ¡ acessÃ­vel na porta 11434.

## ğŸ“¹ VÃ­deo de Teste

Veja o vÃ­deo testando a aplicaÃ§Ã£o: [VÃ­deo de Teste](https://www.youtube.com/watch?v=Wiu-epVUAQo&t=53s)

## ğŸ“œ LicenÃ§a

Este projeto estÃ¡ licenciado sob a LicenÃ§a MIT. Veja o arquivo `LICENSE` para mais detalhes. ğŸ“œ

